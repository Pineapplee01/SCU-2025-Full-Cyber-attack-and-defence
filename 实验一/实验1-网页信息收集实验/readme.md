# Lab 1: Web-Info Collection Agent  
&gt; 2025 Fall Â· PKU AI-Enabled Programming  
&gt; Author: [@Pan-Yunze](https://github.com/Pan-Yunze) | ID: 2023141530019

---

## ğŸ¯ Purpose  
1. Master the **LangGraph ReAct** pattern to build a reliable LLM-powered agent.  
2. Understand **HTTP + HTML basics** and implement two missing tools (`get_webpage_info`, `analyze_webpage`).  
3. Enforce a **â€œmust-call-toolâ€** policy so the model never hallucinates web data.

---

## ğŸ§± Agent Architecture

```text
agent.py â†’ create LLM â†’ bind tools â†’ ReAct-Agent â†’ chat loop

| Module         | What it does                                                                     |
| -------------- | -------------------------------------------------------------------------------- |
| **LLM init**   | `ChatOpenAI(model='deepseek-chat', base_url='https://api.deepseek.com')`         |
| **Prompt**     | Lists 3 tools + hard rule: *â€œAny web-related request MUST call a toolâ€*.         |
| **ReAct loop** | `create_react_agent(llm, tools, prompt, debug=True)`                             |
| **Guard**      | If keywords like `ç½‘é¡µï½œhtmlï½œmeta` trigger but no tool used â†’ reject & ask for URL. |

##ğŸŒ HTTP & HTML Primer
| Layer    | Key Points                                                                               |
| -------- | ---------------------------------------------------------------------------------------- |
| **HTTP** | Req-Line + Headers + Body â†’ Resp-Line (status) + Headers + HTML bytes                    |
| **HTML** | `<html>` â†’ `<head>` (meta, title, canonical) + `<body>` (header/nav/main/article/footer) |
| **DOM**  | Nested tags â†’ tree; CSS selectors style it; depth = longest parent chain                 |

ğŸ”§ Tool Cheat-Sheet
è¡¨æ ¼
å¤åˆ¶
Tool	In one sentence	How
search_subdomains	Grab sub-domains from Bing	Query domain:example.com, scrape <h2><a href="">, extract urlparse(link).netloc
get_webpage_info	Get â€œID cardâ€ of a page	requests â†’ status code; BeautifulSoup â†’ <title>, <meta>, <link rel="canonical">
analyze_webpage	Give the page a â€œhealth checkâ€	Parse DOM â†’ total / unique tags, max depth, <main> â†’ <article> â†’ <body> fallback, count h1-3/p & links, 200-char snippet
ğŸ’» Implementation Highlights
1ï¸âƒ£ get_webpage_info
Python
å¤åˆ¶
resp  = requests.get(url, timeout=10)
soup  = BeautifulSoup(resp.text, 'lxml')

data = {
    "normalized_url": f"{parsed.scheme}://{parsed.netloc}{parsed.path}",
    "http_status"   : resp.status_code,
    "title"         : soup.title.string or "N/A",
    "description"   : soup.find("meta", attrs={"name":"description"})["content"] or "",
    "meta"          : {m.get("name") or m.get("property"): m["content"]
                       for m in soup.find_all("meta") if m.get("content")}
}
2ï¸âƒ£ analyze_webpage
Python
å¤åˆ¶
tags = soup.find_all(True)
data = {
    "dom_summary": {
        "total_tags"  : len(tags),
        "unique_tags" : len({t.name for t in tags}),
        "max_depth"   : max(len(list(t.parents)) for t in tags)
    },
    "main_content_strategy": "<main> â†’ <article> â†’ <body>",
    "important_node_count" : len(soup.find_all(["h1","h2","h3","p"])),
    "link_count"           : len(soup.find_all("a", href=True)),
    "main_text_snippet"    : main_text[:200]
}
ğŸ•¹ï¸ Quick Start
bash
å¤åˆ¶
# 1. clone & install
git clone https://github.com/YOUR_ID/web-info-agent.git
cd web-info-agent
pip install -r requirements.txt

# 2. add key (or export it)
export OPENAI_API_KEY="sk-xxx"

# 3. run
python agent.py
Example chats
å¤åˆ¶
>> æœç´¢ qq.com çš„å­åŸŸå
['https://im.qq.com', 'https://mail.qq.com', ...]

>> è·å–ç½‘é¡µä¿¡æ¯ https://www.baidu.com
HTTP 200 | æ ‡é¢˜: ç™¾åº¦ä¸€ä¸‹ï¼Œä½ å°±çŸ¥é“ | description: å…¨çƒé¢†å…ˆçš„ä¸­æ–‡æœç´¢å¼•æ“ ...

>> åˆ†æç½‘é¡µç»“æ„ https://www.baidu.com
DOM èŠ‚ç‚¹ 634 ä¸ªï¼Œæ·±åº¦ 15ï¼Œä¸»å†…å®¹åŒº<body>ï¼Œé‡è¦èŠ‚ç‚¹ 68ï¼Œé“¾æ¥ 43 ...
ğŸ“Š Fun Facts
ç™¾åº¦é¦–é¡µæ²¡æœ‰ <main> / <article>ï¼Œä¸»å†…å®¹ç­–ç•¥è¢«è¿«å›é€€åˆ° <body>ã€‚
éƒ¨åˆ†ç«™ç‚¹ï¼ˆim.qq.comï¼‰è¿”å› 403ï¼Œä½† Server å¤´ä»æš´éœ² nginxã€‚
Bing å­åŸŸååˆ—è¡¨æ¯æ¬¡ç•¥æœ‰ä¸åŒï¼Œä¸ Cookie ä¼šè¯æœ‰å…³ã€‚
ğŸ“ File Map
å¤åˆ¶
web-info-agent/
â”œâ”€ agent.py          # ä¸»å…¥å£ï¼ˆLLM + ReAct loopï¼‰
â”œâ”€ tools/
â”‚  â””â”€ web_tools.py   # ä¸‰ä¸ªå·¥å…·çš„å®ç°
â”œâ”€ requirements.txt  # ä¾èµ–
â””â”€ README.md         # this file